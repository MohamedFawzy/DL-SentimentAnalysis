{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.4\n",
      "3.0.2\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(mp.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download , unzip and untar files in an automated way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_FILENAME = 'imdbReviews.tar.gz'\n",
    "\n",
    "def download_file(url_path):\n",
    "    \n",
    "    if not os.path.exists(DOWNLOAD_FILENAME):\n",
    "        filename , _ = urllib.request.urlretrieve(url_path, DOWNLOAD_FILENAME)\n",
    "    \n",
    "    print('Found and verified file from this path : ', url_path)\n",
    "    print('Download file: ', DOWNLOAD_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_REGEX = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def get_reviews(dirname, positive=True):\n",
    "    label = 1 if positive else 0\n",
    "    \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    \n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(dirname + filename, 'r+') as f:\n",
    "                \n",
    "                review = f.read()\n",
    "                review = review.lower().replace(\"<br />\", \" \")\n",
    "                review = re.sub(TOKEN_REGEX, \"\", review)\n",
    "                \n",
    "                reviews.append(review)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_data():\n",
    "    if not os.path.exists('aclImdb'):\n",
    "        with tarfile.open(DOWNLOAD_FILENAME) as tar:\n",
    "            tar.extractall()\n",
    "            tar.close()\n",
    "    \n",
    "    positive_reviews, positive_labels = get_reviews(\"aclImdb/train/pos/\", positive=True)\n",
    "    negative_reviews, negative_labels = get_reviews(\"aclImdb/train/neg/\", positive=False)\n",
    "    \n",
    "    data = positive_reviews + negative_reviews\n",
    "    labels = positive_labels + negative_labels\n",
    "    \n",
    "    return labels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified file from this path :  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Download file:  imdbReviews.tar.gz\n"
     ]
    }
   ],
   "source": [
    "URL_PATH = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "download_file(URL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = extract_labels_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem imagine a movie where joe piscopo is actually funny maureen stapleton is a scene stealer the moroni character is an absolute scream watch for alan the skipper hale jr as a police sgt',\n",
       " 'bizarre horror movie filled with famous faces but stolen by cristina raines later of tvs flamingo road as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the gateway to hell the scenes with raines modeling are very well captured the mood music is perfect deborah raffin is charming as cristinas pal but when raines moves into a creepy brooklyn heights brownstone inhabited by a blind priest on the top floor things really start cooking the neighbors including a fantastically wicked burgess meredith and kinky couple sylvia miles  beverly dangelo are a diabolical lot and eli wallach is great fun as a wily police detective the movie is nearly a crosspollination of rosemarys baby and the exorcistbut what a combination based on the bestseller by jeffrey konvitz the sentinel is entertainingly spooky full of shocks brought off well by director michael winner who mounts a thoughtfully downbeat ending with skill 12 from ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n"
     ]
    }
   ],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in data])\n",
    "print(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_document_length = min([len(x.split(\" \")) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233.77672\n"
     ]
    }
   ],
   "source": [
    "average_document_length = sum(len(x.split(\" \")) for x in data) / len(data)\n",
    "print(average_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-b8ca4e758ecb>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = tf.data.VocabularyProcessor(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array(list(vocab_processor.fit_transform(data)))\n",
    "\n",
    "y_output = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111526\n"
     ]
    }
   ],
   "source": [
    "vocabluary_size = len(vocab_processor.vocabulary_)\n",
    "print(vocabluary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['its a strange feeling to sit alone in a theater occupied by parents and their rollicking kids i felt like instead of a movie ticket i should have been given a nambla membership  based upon thomas rockwells respected book how to eat fried worms starts like any childrens story moving to a new town the new kid fifth grader billy forrester was once popular but has to start anew making friends is never easy especially when the only prospect is poindexter adam or erica who at 4 12 feet is a giant  further complicating things is joe the bully his freckled face and sleeveless shirts are daunting he antagonizes kids with the death ring a crackerjack ring that is rumored to kill you if youre punched with it but not immediately no the death ring unleashes a poison that kills you in the eight grade  joe and his axis of evil welcome billy by smuggling a handful of slimy worms into his thermos once discovered billy plays it cool swearing that he eats worms all the time then he throws them at joes face ewww to win them over billy reluctantly bets that he can eat 10 worms fried boiled marinated in hot sauce squashed and spread on a peanut butter sandwich each meal is dubbed an exotic name like the radioactive slime delight in which the kids finally live out their dream of microwaving a living organism  if youve ever met me youll know that i have an uncontrollably hearty laugh i felt like a creep erupting at a toddler whining that his dilly dick hurts but fried worms is wonderfully disgusting like a grated farrelly brothers film it is both vomitous and delightful  writerdirector bob dolman is also a savvy storyteller to raise the stakes the worms must be consumed by 7 pm in addition billy holds a dark secret he has an ultrasensitive stomach  dolman also has a keen sense of perspective with such accuracy he draws on childrens insecurities and tendency to exaggerate mundane dilemmas  if you were to hyperbolize this movie the way kids do their quandaries you will see that it is essentially about war freedomfighter and freedomhater use pubescent boys as pawns in proxy wars only to learn a valuable lesson in unity international leaders can learn a thing or two about global peacekeeping from fried worms  at the end of the film i was comforted when two chaperoning mothers behind me looked at each other with befuddlement and agreed that was a great movie great now i wont have to register myself in any lawful databases',\n",
       " 'you probably all already know this by now but 5 additional episodes never aired can be viewed on abccom ive watched a lot of television over the years and this is possibly my favorite show ever its a crime that this beautifully written and acted show was canceled the actors that played laura whit carlos mae damian anya and omg steven caseman  are all incredible and so natural in those roles even the kids are great wonderful show so sad that its gone of course i wonder about the reasons it was canceled there is no way ill let myself believe that ms moynahans pregnancy had anything to do with it it was in the perfect time slot in this market ive watched all the episodes again on abccom  i hope they all come out on dvd some day thanks for reading']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[186,   2, 187, 188,  66, 189, 190, 191,   2, 192, 193,  51, 194,\n",
       "        110, 195, 196, 197, 176, 198, 199, 200,  12,   2,   3, 201, 176,\n",
       "        202, 203, 204, 205,   2, 206, 207, 131, 208, 209, 210, 211, 212,\n",
       "        213,  66, 214, 215, 216, 217, 199, 218, 219, 220, 221,  66,   2,\n",
       "        222, 223,  29, 222, 224, 225, 226, 227, 228, 158, 229, 230,  49,\n",
       "        231,  66, 102, 232, 233, 234,  22, 235, 236, 237,  87,  29, 163,\n",
       "        238,  22, 239, 240, 241, 242,  64, 243, 244, 150, 245,  22,   2,\n",
       "        246, 247, 248, 100,  22,  20,  29, 249, 250, 251, 252, 110, 253,\n",
       "        254,  10, 255, 256, 257, 197,  46,  29, 258, 259,   2, 260, 259,\n",
       "          4,  22, 261,  66, 262, 263, 153, 264, 265,  46, 266,  49, 267,\n",
       "        268,   6,  29, 258, 259, 269,   2, 270,   4, 271, 263, 191,  29,\n",
       "        272, 273,  20, 110, 250, 274,  12, 275, 276, 227,  51, 277,   2,\n",
       "        278,  12, 279, 216,  89, 250, 280, 229, 281, 227, 282, 266, 283,\n",
       "        284,   4, 256, 285, 216, 286,  29, 287, 288, 256, 289, 290, 243,\n",
       "        291, 252, 292,  66, 293, 290, 294, 227, 295, 296,   4, 256, 297,\n",
       "        214, 298, 216, 215, 299, 300, 191, 301, 302, 303, 110, 304,  97,\n",
       "          2, 305, 306, 307, 308, 309,  22, 310,  32, 311, 312, 199,  29,\n",
       "        313, 314, 315, 191, 316,  29, 197, 317, 318, 169, 195, 319,  12,\n",
       "        320,   2, 321, 322, 153, 323, 324, 325, 167, 326, 327,   4, 176,\n",
       "        203,  32, 328],\n",
       "       [263, 416, 286, 417, 327,  16,  51, 410,  49, 418, 419, 420, 235,\n",
       "        421, 297, 355, 422,  97, 423, 424, 425,   2,  11,  12, 426, 294,\n",
       "         29, 427, 110,  16,  22, 428, 160, 161, 429, 324, 186,   2, 430,\n",
       "          4,  16, 431, 432, 110, 433, 429, 158, 434,  29, 435,   4, 436,\n",
       "        437, 438, 439, 440, 441, 442, 110, 443, 444, 445,  10, 286, 446,\n",
       "        110, 447, 448, 191, 449, 450, 451,  29, 197,  10, 120, 159, 429,\n",
       "        447, 452,   4, 186, 453,  12, 454, 176, 455, 382,  29, 456, 266,\n",
       "        158, 434,   8,  22,   6, 170, 457, 458, 413, 459,   4, 460, 461,\n",
       "        462, 463, 464,  66, 378,  46, 266, 266, 158, 191,  29,  81, 287,\n",
       "        465, 191,  16, 466, 424, 425, 286,  29, 420, 172,  97, 423, 176,\n",
       "        467, 181, 286, 468, 169,  97, 469, 470, 471, 472,   1, 473,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(x_data)))\n",
    "\n",
    "x_shuffled = x_data[shuffle_indices]\n",
    "y_shuffled = y_output[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = 5000\n",
    "TOTAL_DATA = 6000\n",
    "\n",
    "train_data = x_shuffled[:TRAIN_DATA]\n",
    "train_target = y_shuffled[:TRAIN_DATA]\n",
    "\n",
    "test_data = x_shuffled[TRAIN_DATA:TOTAL_DATA]\n",
    "test_target = y_shuffled[TRAIN_DATA:TOTAL_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, MAX_SEQUENCE_LENGTH])\n",
    "y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 20\n",
    "batch_size = 25\n",
    "embedding_size = 50\n",
    "max_label = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = tf.Variable(tf.random_uniform([vocabluary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "embeddings = tf.nn.embedding_lookup(embedding_matrix, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(111526, 50) dtype=float32_ref>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(?, 250, 50) dtype=float32>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(embedding_size)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (encoding, _) = tf.nn.dynamic_rnn(lstmCell, embeddings, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(encoding, max_label, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test loss: 0.7, Test Acc: 0.48\n",
      "Epoch: 2, Test loss: 0.75, Test Acc: 0.528\n",
      "Epoch: 3, Test loss: 0.68, Test Acc: 0.73\n",
      "Epoch: 4, Test loss: 0.79, Test Acc: 0.795\n",
      "Epoch: 5, Test loss: 0.88, Test Acc: 0.806\n",
      "Epoch: 6, Test loss: 1.0, Test Acc: 0.818\n",
      "Epoch: 7, Test loss: 1.1, Test Acc: 0.814\n",
      "Epoch: 8, Test loss: 1.1, Test Acc: 0.812\n",
      "Epoch: 9, Test loss: 1.2, Test Acc: 0.82\n",
      "Epoch: 10, Test loss: 1.3, Test Acc: 0.819\n",
      "Epoch: 11, Test loss: 1.3, Test Acc: 0.821\n",
      "Epoch: 12, Test loss: 1.4, Test Acc: 0.825\n",
      "Epoch: 13, Test loss: 1.5, Test Acc: 0.822\n",
      "Epoch: 14, Test loss: 1.5, Test Acc: 0.821\n",
      "Epoch: 15, Test loss: 1.6, Test Acc: 0.822\n",
      "Epoch: 16, Test loss: 1.6, Test Acc: 0.818\n",
      "Epoch: 17, Test loss: 1.7, Test Acc: 0.817\n",
      "Epoch: 18, Test loss: 1.7, Test Acc: 0.816\n",
      "Epoch: 19, Test loss: 1.7, Test Acc: 0.815\n",
      "Epoch: 20, Test loss: 1.8, Test Acc: 0.817\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(num_epoches):\n",
    "        num_batches = int(len(train_data) // batch_size) + 1\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            \n",
    "            min_ix = i * batch_size\n",
    "            max_ix = np.min([len(train_data), ((i+1) * batch_size)])\n",
    "            \n",
    "            x_train_batch = train_data[min_ix: max_ix]\n",
    "            y_train_batch = train_target[min_ix:max_ix]\n",
    "            \n",
    "            train_dict = {x: x_train_batch, y: y_train_batch}\n",
    "            session.run(train_step, feed_dict=train_dict)\n",
    "            \n",
    "            train_loss, train_acc = session.run([loss, accuracy], feed_dict=train_dict)\n",
    "            \n",
    "        test_dict = {x: test_data, y: test_target}\n",
    "        test_loss, test_acc = session.run([loss, accuracy], feed_dict=test_dict)\n",
    "        print('Epoch: {}, Test loss: {:.2}, Test Acc: {:.5}'.format(epoch+1, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
