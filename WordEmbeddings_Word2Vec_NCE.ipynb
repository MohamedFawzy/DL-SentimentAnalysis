{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.4\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_FILENAME = \"SampleText.zip\"\n",
    "def download(url_path, expected_bytes):\n",
    "    if not os.path.exists(DOWNLOAD_FILENAME):\n",
    "        filename, _ = urllib.request.urlretrieve(url_path, DOWNLOAD_FILENAME)\n",
    "        \n",
    "        statinfo = os.stat(DOWNLOAD_FILENAME)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print(\"Found and verified file from this path : \", url_path)\n",
    "            print(\"Downloaded filed: \", DOWNLOAD_FILENAME)\n",
    "        else:\n",
    "            print(statinfo.st_size)\n",
    "            raise Exception(\"Failed to verify file from \" + url_path + \" Can you get with a browser?\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words():\n",
    "    with zipfile.ZipFile(DOWNLOAD_FILENAME) as f:\n",
    "        firstfile = f.namelist()[0]\n",
    "        filestring = tf.compat.as_str(f.read(firstfile))\n",
    "        words = filestring.split()\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PATH = 'http://mattmahoney.net/dc/text8.zip'\n",
    "FILE_SIZE = 31344016\n",
    "download(URL_PATH, FILE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'including',\n",
       " 'the',\n",
       " 'diggers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'english',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sans',\n",
       " 'culottes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    word_counts = [['UNKOWN', -1]]\n",
    "    \n",
    "    counter = collections.Counter(words)\n",
    "    word_counts.extend(counter.most_common(n_words-1))\n",
    "    \n",
    "    dictonary = dict()\n",
    "    \n",
    "    for word, _ in word_counts:\n",
    "        dictonary[word] = len(dictonary)\n",
    "    \n",
    "    word_indexes = list()\n",
    "    unkown_count = 0\n",
    "    for word in words:\n",
    "        if word in dictonary:\n",
    "            index = dictonary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unkown_count += 1\n",
    "        \n",
    "        word_indexes.append(index)\n",
    "    \n",
    "    word_counts[0][1] = unkown_count\n",
    "    \n",
    "    reversed_dictonary = dict(zip(dictonary.values(), dictonary.keys()))\n",
    "    \n",
    "    return word_counts, word_indexes, dictonary, reversed_dictonary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_SIZE = 10000\n",
    "word_counts, word_indexes, dictonary , reversed_dictonary = build_dataset(voc, VOC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNKOWN', 1737307],\n",
       " ('the', 1061396),\n",
       " ('of', 593677),\n",
       " ('and', 416629),\n",
       " ('one', 411764),\n",
       " ('in', 372201),\n",
       " ('a', 325873),\n",
       " ('to', 316376),\n",
       " ('zero', 264975),\n",
       " ('nine', 250430)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moments : 7008\n",
      "state : 94\n",
      "confusion : 3383\n",
      "policy : 739\n",
      "absinthe : 9441\n",
      "houses : 2037\n",
      "winter : 1444\n",
      "bengals : 7699\n",
      "sk : 5505\n",
      "cowboy : 8141\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for key in random.sample(list(dictonary), 10):\n",
    "    print(key, \":\", dictonary[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9515 : ethnicity\n",
      "1070 : magazine\n",
      "4681 : maxwell\n",
      "6763 : kyoto\n",
      "7174 : click\n",
      "6927 : bsd\n",
      "2613 : anglo\n",
      "1617 : begin\n",
      "2220 : rail\n",
      "4221 : mechanisms\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for key in random.sample(list(reversed_dictonary),10):\n",
    "    print(key, \":\", reversed_dictonary[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(word_indexes, batch_size, num_skips, skip_window):\n",
    "    global global_index\n",
    "    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <=  2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span-1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window] # the input word\n",
    "            labels[i * num_skips + j, 0] = buffer[target] # the context words\n",
    "        \n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)\n",
    "    \n",
    "    global_index = (global_index + len(word_indexes) - span) % len(word_indexes)\n",
    "    \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(word_indexes, 10, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    2, 3134, 3134,   46,   46,   59,   59,  156,  156],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3081],\n",
       "       [  46],\n",
       "       [ 156],\n",
       "       [   6],\n",
       "       [  12],\n",
       "       [ 742],\n",
       "       [ 477],\n",
       "       [  46],\n",
       "       [3134],\n",
       "       [ 128]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of : originated\n",
      "of : first\n",
      "abuse : against\n",
      "abuse : a\n",
      "first : as\n",
      "first : working\n",
      "used : class\n",
      "used : first\n",
      "against : abuse\n",
      "against : early\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(reversed_dictonary[batch[i]], \":\", reversed_dictonary[labels[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16\n",
    "valid_window = 100\n",
    "\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 50 # no of hidden layers neuorns\n",
    "skip_window = 2\n",
    "num_skips = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "\n",
    "    tf.random_uniform([VOC_SIZE, embedding_size], -1.0, 1.0))\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(10000, 50) dtype=float32_ref>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(128, 50) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(tf.truncated_normal([VOC_SIZE, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "nce_bias = tf.Variable(tf.zeros(VOC_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss = tf.reduce_mean(\n",
    "                    tf.nn.nce_loss(weights=nce_weights,\n",
    "                                   biases=nce_bias,\n",
    "                                   labels=train_labels,\n",
    "                                   inputs=embed,\n",
    "                                   num_sampled=num_samples,\n",
    "                                   num_classes=VOC_SIZE\n",
    "                                  ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "\n",
    "normalized_embeddings = embeddings / l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_1:0' shape=(16, 50) dtype=float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 200001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0 :  249.76617431640625\n",
      "Nearest to no:  helps poll neo pixel elector emphasis pace scripture\n",
      "Nearest to states:  conviction certainly physicists parents spellings heard bees protests\n",
      "Nearest to be:  zeppelin bachelor pace scotland bankruptcy answered pakistani zones\n",
      "Nearest to these:  latex coup holiday neighbouring served confession euclid traits\n",
      "Nearest to may:  coded sullivan underlying drafted alloys increases root we\n",
      "Nearest to was:  hebrew soap adds gain principally missions end friedrich\n",
      "Nearest to when:  codified farther especially editions trotsky gary included demonstrate\n",
      "Nearest to such:  wine flanders updated burkina violations closure bag chomsky\n",
      "Nearest to i:  wonder consent dakota jay patterns particular cannon drinking\n",
      "Nearest to six:  questions experts stressed ribbentrop ca espionage official publications\n",
      "Nearest to nine:  eighty producer registered langle database q library session\n",
      "Nearest to there:  deposed intense injection onset faced particles freight cat\n",
      "Nearest to called:  believe regulate ciphers metadata prepare suicide reproductive constantly\n",
      "Nearest to during:  bees methodology manager holder americas evans cuban drawn\n",
      "Nearest to more:  cathode biography prove acupuncture flowing cheap goes bradley\n",
      "Nearest to for:  jumps thomas reed cyrillic affair seven directory relativity\n",
      "\n",
      "\n",
      "Average loss at step 2000 :  103.52480213546752\n",
      "Average loss at step 4000 :  46.131001454591754\n",
      "Average loss at step 6000 :  31.903536550998687\n",
      "Average loss at step 8000 :  24.080755222558974\n",
      "Average loss at step 10000 :  18.828741429328918\n",
      "Nearest to no:  they often some be after through sea nine\n",
      "Nearest to states:  would first other six during four at which\n",
      "Nearest to be:  that on nine war an it s many\n",
      "Nearest to these:  john may four best he example are that\n",
      "Nearest to may:  are these he eight s as group six\n",
      "Nearest to was:  an end that were all at being six\n",
      "Nearest to when:  its einstein especially can eight all four major\n",
      "Nearest to such:  two not s it six with are nine\n",
      "Nearest to i:  it time africa which an gore war anti\n",
      "Nearest to six:  eight four for an has five nine three\n",
      "Nearest to nine:  zero six agave five include after seven eight\n",
      "Nearest to there:  agave under six eight on seven history american\n",
      "Nearest to called:  would have at five six up over zero\n",
      "Nearest to during:  been also six states over its would groups\n",
      "Nearest to more:  agave five that same had up as but\n",
      "Nearest to for:  from seven six other been have three can\n",
      "\n",
      "\n",
      "Average loss at step 12000 :  16.10271990644932\n",
      "Average loss at step 14000 :  13.777330592155456\n",
      "Average loss at step 16000 :  11.51650825881958\n",
      "Average loss at step 18000 :  10.182031384944915\n",
      "Average loss at step 20000 :  10.454942835569382\n",
      "Nearest to no:  often sea they through live field after theory\n",
      "Nearest to states:  would during first himself other based groups house\n",
      "Nearest to be:  whose war that open last scotland on america\n",
      "Nearest to these:  john best example may similar language served general\n",
      "Nearest to may:  group we population title central apple these country\n",
      "Nearest to was:  end hebrew several being richard were group between\n",
      "Nearest to when:  especially einstein its included major m can international\n",
      "Nearest to such:  civil not external them story north two rather\n",
      "Nearest to i:  particular anti africa gore time development consent england\n",
      "Nearest to six:  eight four agave so person time main like\n",
      "Nearest to nine:  include einstein six agave after bc zero seven\n",
      "Nearest to there:  agave under rand actor especially history book way\n",
      "Nearest to called:  would up over believe aikido computer information have\n",
      "Nearest to during:  been groups over states also years would use\n",
      "Nearest to more:  agave same up south lincoln being word had\n",
      "Nearest to for:  from asia seven take thomas immediately other early\n",
      "\n",
      "\n",
      "Average loss at step 22000 :  9.0833841599226\n",
      "Average loss at step 24000 :  8.718395026922225\n",
      "Average loss at step 26000 :  8.12393552672863\n",
      "Average loss at step 28000 :  7.700429291725158\n",
      "Average loss at step 30000 :  7.101056567788124\n",
      "Nearest to no:  sea often live field through alfred theory nearly\n",
      "Nearest to states:  himself would during house electron groups wrote based\n",
      "Nearest to be:  whose scotland that open last massachusetts war selected\n",
      "Nearest to these:  john best served example italian similar things language\n",
      "Nearest to may:  group title population central we food apple city\n",
      "Nearest to was:  hebrew end several and richard being ability missions\n",
      "Nearest to when:  especially einstein included major international m o early\n",
      "Nearest to such:  civil story albert catholic external rather north them\n",
      "Nearest to i:  particular consent anti gore development africa longer press\n",
      "Nearest to six:  net person oxford whether making force eight ca\n",
      "Nearest to nine:  include one give einstein bc zero water births\n",
      "Nearest to there:  agave actor rand under especially way book prior\n",
      "Nearest to called:  believe would aikido up computer austrian information named\n",
      "Nearest to during:  been groups states over years rand gold word\n",
      "Nearest to more:  same agave biography half south goes acupuncture original\n",
      "Nearest to for:  from immediately asia take unique record character now\n",
      "\n",
      "\n",
      "Average loss at step 32000 :  7.031172266602516\n",
      "Average loss at step 34000 :  6.697262593507767\n",
      "Average loss at step 36000 :  6.626483675479889\n",
      "Average loss at step 38000 :  6.458735927462578\n",
      "Average loss at step 40000 :  6.122160271048545\n",
      "Nearest to no:  sea helps live often field el nearly alfred\n",
      "Nearest to states:  himself electron house wrote disease certainly medicine heard\n",
      "Nearest to be:  whose scotland that companion selected hellenistic massachusetts open\n",
      "Nearest to these:  served john best italian example things coup similar\n",
      "Nearest to may:  title group we central population food apple nasa\n",
      "Nearest to was:  hebrew and end richard several missions being ability\n",
      "Nearest to when:  especially einstein included described o major international m\n",
      "Nearest to such:  wine civil story rate burkina catholic albert ten\n",
      "Nearest to i:  consent particular influential longer anti development gore press\n",
      "Nearest to six:  net oxford person ca four making eight feet\n",
      "Nearest to nine:  one include zero give count einstein births bc\n",
      "Nearest to there:  agave actor rand under especially prior way book\n",
      "Nearest to called:  believe aikido computer austrian deaths named suicide would\n",
      "Nearest to during:  groups containing rand gold been risk states years\n",
      "Nearest to more:  biography same goes acupuncture prove half agave lord\n",
      "Nearest to for:  from unique immediately take record asia character in\n",
      "\n",
      "\n",
      "Average loss at step 42000 :  6.053611286878586\n",
      "Average loss at step 44000 :  5.913157403111458\n",
      "Average loss at step 46000 :  5.823525295853615\n",
      "Average loss at step 48000 :  5.668954278111458\n",
      "Average loss at step 50000 :  5.540395635962486\n",
      "Nearest to no:  helps sea live el often step nearly field\n",
      "Nearest to states:  himself electron certainly heard disease wrote house medicine\n",
      "Nearest to be:  whose companion hellenistic scotland that selected massachusetts sons\n",
      "Nearest to these:  served italian best coup holiday john example things\n",
      "Nearest to may:  title group central population we food apple physician\n",
      "Nearest to was:  and hebrew is channels richard missions friedrich were\n",
      "Nearest to when:  especially included einstein described o maryland uk anchor\n",
      "Nearest to such:  wine civil rate story burkina updated albert catholic\n",
      "Nearest to i:  consent influential particular wonder longer patterns drinking anti\n",
      "Nearest to six:  net oxford four eight person ca feet paris\n",
      "Nearest to nine:  one include zero give count six five baptism\n",
      "Nearest to there:  actor agave rand cat prior under particles especially\n",
      "Nearest to called:  believe aikido suicide austrian computer deaths named issued\n",
      "Nearest to during:  containing rand gold risk groups apparent manager been\n",
      "Nearest to more:  biography goes prove acupuncture same half agave lord\n",
      "Nearest to for:  from in unique immediately and record character take\n",
      "\n",
      "\n",
      "Average loss at step 52000 :  5.411650134205818\n",
      "Average loss at step 54000 :  5.427768917798996\n",
      "Average loss at step 56000 :  5.448226014375686\n",
      "Average loss at step 58000 :  5.350464254021644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 60000 :  5.296778514623642\n",
      "Nearest to no:  helps sea el live often poll step nearly\n",
      "Nearest to states:  himself certainly electron heard disease parents wrote medicine\n",
      "Nearest to be:  whose hellenistic companion selected scotland that massachusetts sons\n",
      "Nearest to these:  served italian coup holiday best pascal john doctrine\n",
      "Nearest to may:  title group central we physician population food apple\n",
      "Nearest to was:  is hebrew and were channels missions richard friedrich\n",
      "Nearest to when:  especially included einstein described maryland o uk anchor\n",
      "Nearest to such:  wine civil rate burkina updated wwii story votes\n",
      "Nearest to i:  consent influential particular wonder patterns drinking longer anti\n",
      "Nearest to six:  net four eight oxford five three ca feet\n",
      "Nearest to nine:  one zero include five six seven give count\n",
      "Nearest to there:  actor agave rand cat particles injection intense prior\n",
      "Nearest to called:  believe aikido suicide austrian deaths computer issued named\n",
      "Nearest to during:  bees containing rand apparent risk gold manager groups\n",
      "Nearest to more:  biography goes prove acupuncture same half lord christ\n",
      "Nearest to for:  from in and unique immediately record character asia\n",
      "\n",
      "\n",
      "Average loss at step 62000 :  5.333864700317383\n",
      "Average loss at step 64000 :  5.2607458885908125\n",
      "Average loss at step 66000 :  5.027440111398697\n",
      "Average loss at step 68000 :  5.271014391899109\n",
      "Average loss at step 70000 :  5.1589672383069995\n",
      "Nearest to no:  helps el sea live poll often step nearly\n",
      "Nearest to states:  certainly himself heard electron parents disease wrote medicine\n",
      "Nearest to be:  hellenistic companion whose selected scotland that sons massachusetts\n",
      "Nearest to these:  served coup italian holiday best pascal doctrine john\n",
      "Nearest to may:  physician title we group apple animated food population\n",
      "Nearest to was:  is hebrew were missions channels friedrich richard and\n",
      "Nearest to when:  especially included einstein codified maryland described farther uk\n",
      "Nearest to such:  wine wwii civil rate updated burkina story votes\n",
      "Nearest to i:  consent influential wonder patterns particular drinking longer jay\n",
      "Nearest to six:  net four eight five three oxford nine paris\n",
      "Nearest to nine:  one zero six five seven wa eight count\n",
      "Nearest to there:  actor agave cat rand injection particles intense wonder\n",
      "Nearest to called:  believe aikido suicide austrian issued deaths computer named\n",
      "Nearest to during:  bees containing apparent rand risk americas cuban gold\n",
      "Nearest to more:  biography goes prove acupuncture half cathode same maintains\n",
      "Nearest to for:  from in and unique immediately of record by\n",
      "\n",
      "\n",
      "Average loss at step 72000 :  5.068932363271713\n",
      "Average loss at step 74000 :  5.14049773979187\n",
      "Average loss at step 76000 :  5.067390096306801\n",
      "Average loss at step 78000 :  5.019563319563866\n",
      "Average loss at step 80000 :  4.915968080639839\n",
      "Nearest to no:  helps el sea poll often live step nearly\n",
      "Nearest to states:  certainly himself heard electron parents disease wrote medicine\n",
      "Nearest to be:  hellenistic companion selected whose scotland that sons useful\n",
      "Nearest to these:  served coup italian holiday best pascal doctrine napoleon\n",
      "Nearest to may:  physician title we apple group animated performer soap\n",
      "Nearest to was:  is hebrew were missions channels friedrich richard labour\n",
      "Nearest to when:  included especially einstein codified maryland described farther uk\n",
      "Nearest to such:  wine updated wwii rate civil burkina parties votes\n",
      "Nearest to i:  consent influential patterns wonder dakota drinking jay particular\n",
      "Nearest to six:  four net eight five three nine oxford paris\n",
      "Nearest to nine:  one zero six five seven eight wa count\n",
      "Nearest to there:  actor injection agave cat particles rand wonder intense\n",
      "Nearest to called:  believe aikido suicide austrian issued deaths computer named\n",
      "Nearest to during:  bees containing apparent americas rand risk cuban manager\n",
      "Nearest to more:  biography goes prove acupuncture cathode maintains half lord\n",
      "Nearest to for:  from in and unique immediately of record by\n",
      "\n",
      "\n",
      "Average loss at step 82000 :  4.9823969149589535\n",
      "Average loss at step 84000 :  5.010792878985405\n",
      "Average loss at step 86000 :  4.969932804822922\n",
      "Average loss at step 88000 :  4.9563446267843245\n",
      "Average loss at step 90000 :  4.9390627917051315\n",
      "Nearest to no:  helps el often sea poll live step nearly\n",
      "Nearest to states:  certainly heard himself electron parents conviction wrote disease\n",
      "Nearest to be:  hellenistic companion selected whose scotland sons useful pakistani\n",
      "Nearest to these:  served coup italian holiday best pascal doctrine napoleon\n",
      "Nearest to may:  physician we title apple soap animated performer group\n",
      "Nearest to was:  is were hebrew missions channels friedrich labour richard\n",
      "Nearest to when:  included especially einstein codified maryland described farther uk\n",
      "Nearest to such:  wine wwii updated rate chomsky burkina parties learned\n",
      "Nearest to i:  consent influential patterns wonder dakota jay drinking longer\n",
      "Nearest to six:  four eight net three five nine one oxford\n",
      "Nearest to nine:  one zero six seven eight five wa count\n",
      "Nearest to there:  actor injection cat wonder agave particles rand intense\n",
      "Nearest to called:  believe aikido suicide issued austrian deaths computer atheism\n",
      "Nearest to during:  bees apparent containing americas rand risk cuban sample\n",
      "Nearest to more:  cathode biography goes prove acupuncture maintains cheap lord\n",
      "Nearest to for:  from in and unique by immediately record of\n",
      "\n",
      "\n",
      "Average loss at step 92000 :  4.947627254247665\n",
      "Average loss at step 94000 :  4.920702034235001\n",
      "Average loss at step 96000 :  4.950924129724503\n",
      "Average loss at step 98000 :  4.903184565424919\n",
      "Average loss at step 100000 :  4.87637863612175\n",
      "Nearest to no:  helps el often poll sea live step nearly\n",
      "Nearest to states:  certainly heard himself electron parents conviction wrote disease\n",
      "Nearest to be:  hellenistic companion selected whose scotland useful pakistani sons\n",
      "Nearest to these:  coup holiday served italian pascal best doctrine napoleon\n",
      "Nearest to may:  physician we soap apple performer animated title fossil\n",
      "Nearest to was:  is were hebrew missions channels friedrich labour richard\n",
      "Nearest to when:  included especially einstein codified maryland farther described loose\n",
      "Nearest to such:  wine chomsky wwii updated rate flanders parties burkina\n",
      "Nearest to i:  consent influential patterns wonder dakota jay drinking junta\n",
      "Nearest to six:  four eight three net five nine one oxford\n",
      "Nearest to nine:  one six zero eight seven five wa three\n",
      "Nearest to there:  actor injection wonder cat particles rand agave intense\n",
      "Nearest to called:  believe aikido suicide issued austrian computer deaths atheism\n",
      "Nearest to during:  bees apparent containing americas rand rebels sample risk\n",
      "Nearest to more:  cathode goes biography prove acupuncture maintains cheap lord\n",
      "Nearest to for:  from in and unique by record immediately of\n",
      "\n",
      "\n",
      "Average loss at step 102000 :  4.860215884685516\n",
      "Average loss at step 104000 :  4.877353979945183\n",
      "Average loss at step 106000 :  4.861302726387978\n",
      "Average loss at step 108000 :  4.8396382786035534\n",
      "Average loss at step 110000 :  4.8618522772789\n",
      "Nearest to no:  helps el often poll sea pixel live step\n",
      "Nearest to states:  certainly heard himself electron conviction parents wrote disease\n",
      "Nearest to be:  hellenistic companion selected scotland whose pakistani useful sons\n",
      "Nearest to these:  coup holiday served italian pascal doctrine best napoleon\n",
      "Nearest to may:  physician we performer animated soap apple sullivan fossil\n",
      "Nearest to was:  is were hebrew missions channels friedrich labour richard\n",
      "Nearest to when:  included especially einstein codified maryland farther loose described\n",
      "Nearest to such:  chomsky wine wwii flanders updated rate parties learned\n",
      "Nearest to i:  consent influential patterns wonder dakota jay drinking junta\n",
      "Nearest to six:  four eight three five net nine one two\n",
      "Nearest to nine:  one eight six seven zero five wa three\n",
      "Nearest to there:  actor injection wonder particles cat rand agave intense\n",
      "Nearest to called:  believe suicide aikido issued austrian computer deaths atheism\n",
      "Nearest to during:  bees apparent americas containing rebels sample rand ethiopia\n",
      "Nearest to more:  cathode goes biography prove acupuncture maintains cheap christ\n",
      "Nearest to for:  from in unique and by record immediately designer\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 112000 :  4.867471558213234\n",
      "Average loss at step 114000 :  4.893861685633659\n",
      "Average loss at step 116000 :  4.822347330570221\n",
      "Average loss at step 118000 :  4.858448916554451\n",
      "Average loss at step 120000 :  4.807747573018074\n",
      "Nearest to no:  helps often el poll sea pixel they nearly\n",
      "Nearest to states:  certainly heard himself electron conviction parents wrote disease\n",
      "Nearest to be:  hellenistic companion selected scotland whose useful pakistani sons\n",
      "Nearest to these:  coup italian holiday served pascal doctrine best napoleon\n",
      "Nearest to may:  physician we performer animated sullivan soap apple fossil\n",
      "Nearest to was:  is were hebrew missions labour channels friedrich richard\n",
      "Nearest to when:  included especially einstein codified maryland farther discs loose\n",
      "Nearest to such:  chomsky wine flanders wwii updated parties rate learned\n",
      "Nearest to i:  consent influential patterns wonder dakota jay junta drinking\n",
      "Nearest to six:  four eight three five net nine one two\n",
      "Nearest to nine:  one eight six seven zero five wa three\n",
      "Nearest to there:  actor injection wonder particles rand cat napoleon deposed\n",
      "Nearest to called:  suicide believe aikido issued computer austrian reproductive deaths\n",
      "Nearest to during:  bees apparent rebels ethiopia sample americas methodology containing\n",
      "Nearest to more:  cathode goes maintains biography acupuncture prove cheap christ\n",
      "Nearest to for:  from in unique by record designer and immediately\n",
      "\n",
      "\n",
      "Average loss at step 122000 :  4.844453635573387\n",
      "Average loss at step 124000 :  4.849151927351952\n",
      "Average loss at step 126000 :  4.798613561153412\n",
      "Average loss at step 128000 :  4.819570186376572\n",
      "Average loss at step 130000 :  4.780789354205131\n",
      "Nearest to no:  helps often el poll they sea nearly drama\n",
      "Nearest to states:  certainly heard himself electron conviction parents tech wrote\n",
      "Nearest to be:  hellenistic companion selected scotland whose pakistani useful sons\n",
      "Nearest to these:  coup italian holiday served pascal doctrine best napoleon\n",
      "Nearest to may:  physician sullivan we performer animated soap fossil apple\n",
      "Nearest to was:  is were hebrew missions labour channels friedrich richard\n",
      "Nearest to when:  included especially einstein codified maryland discs farther loose\n",
      "Nearest to such:  chomsky flanders wine wwii updated parties rate learned\n",
      "Nearest to i:  consent influential patterns wonder dakota jay junta drinking\n",
      "Nearest to six:  four eight three five net nine one seven\n",
      "Nearest to nine:  one eight six seven zero five wa births\n",
      "Nearest to there:  injection actor wonder particles rand cat napoleon deposed\n",
      "Nearest to called:  suicide aikido believe issued computer austrian reproductive atheism\n",
      "Nearest to during:  bees rebels apparent sample methodology ethiopia americas legislature\n",
      "Nearest to more:  cathode maintains goes acupuncture prove biography cheap christ\n",
      "Nearest to for:  from in unique record by designer immediately somewhere\n",
      "\n",
      "\n",
      "Average loss at step 132000 :  4.776383561134338\n",
      "Average loss at step 134000 :  4.835014012694359\n",
      "Average loss at step 136000 :  4.74238843870163\n",
      "Average loss at step 138000 :  4.777514129757881\n",
      "Average loss at step 140000 :  4.78157128727436\n",
      "Nearest to no:  helps often el poll they sea drama nearly\n",
      "Nearest to states:  certainly heard himself electron conviction parents tech spellings\n",
      "Nearest to be:  hellenistic companion selected scotland pakistani whose useful sons\n",
      "Nearest to these:  coup holiday italian served pascal doctrine napoleon best\n",
      "Nearest to may:  physician sullivan we performer animated fossil soap apple\n",
      "Nearest to was:  is were hebrew missions labour friedrich channels being\n",
      "Nearest to when:  included especially einstein codified maryland discs farther loose\n",
      "Nearest to such:  chomsky flanders wwii wine updated parties learned rate\n",
      "Nearest to i:  consent influential patterns wonder dakota jay junta drinking\n",
      "Nearest to six:  four eight three five nine one net seven\n",
      "Nearest to nine:  one eight six seven zero five wa births\n",
      "Nearest to there:  injection actor wonder napoleon rand particles cat deposed\n",
      "Nearest to called:  suicide aikido believe issued computer austrian reproductive atheism\n",
      "Nearest to during:  rebels bees apparent methodology sample ethiopia legislature americas\n",
      "Nearest to more:  cathode maintains goes acupuncture prove biography cheap christ\n",
      "Nearest to for:  from in designer record unique by immediately antiquity\n",
      "\n",
      "\n",
      "Average loss at step 142000 :  4.623085867285728\n",
      "Average loss at step 144000 :  4.8129880856275555\n",
      "Average loss at step 146000 :  4.786546911597252\n",
      "Average loss at step 148000 :  4.808954297065735\n",
      "Average loss at step 150000 :  4.804471230626106\n",
      "Nearest to no:  helps often poll el sea they pixel drama\n",
      "Nearest to states:  certainly heard electron himself conviction parents tech spellings\n",
      "Nearest to be:  hellenistic companion selected scotland pakistani useful whose sons\n",
      "Nearest to these:  coup holiday italian served pascal doctrine napoleon best\n",
      "Nearest to may:  physician sullivan we performer fossil animated soap apple\n",
      "Nearest to was:  is were hebrew missions labour friedrich channels being\n",
      "Nearest to when:  included especially einstein codified maryland discs farther acceleration\n",
      "Nearest to such:  chomsky flanders wwii wine updated parties learned utilize\n",
      "Nearest to i:  consent influential patterns dakota wonder junta jay drinking\n",
      "Nearest to six:  four eight three five one nine seven net\n",
      "Nearest to nine:  one eight six seven zero five wa births\n",
      "Nearest to there:  injection actor wonder napoleon rand particles deposed cat\n",
      "Nearest to called:  suicide aikido believe issued computer reproductive atheism austrian\n",
      "Nearest to during:  rebels methodology sample bees apparent legislature ethiopia americas\n",
      "Nearest to more:  cathode maintains acupuncture goes prove cheap biography christ\n",
      "Nearest to for:  from in designer record unique antiquity immediately by\n",
      "\n",
      "\n",
      "Average loss at step 152000 :  4.766717471003532\n",
      "Average loss at step 154000 :  4.776593877017498\n",
      "Average loss at step 156000 :  4.799185028195381\n",
      "Average loss at step 158000 :  4.787176815867424\n",
      "Average loss at step 160000 :  4.7743044424057\n",
      "Nearest to no:  helps often poll el they pixel sea drama\n",
      "Nearest to states:  certainly heard electron himself conviction parents spellings tech\n",
      "Nearest to be:  hellenistic companion selected scotland useful pakistani whose sons\n",
      "Nearest to these:  coup holiday italian served pascal doctrine napoleon ibn\n",
      "Nearest to may:  physician sullivan we performer fossil animated apple soap\n",
      "Nearest to was:  is were hebrew missions labour friedrich channels being\n",
      "Nearest to when:  included especially einstein codified maryland discs peers acceleration\n",
      "Nearest to such:  chomsky flanders wwii wine updated parties learned utilize\n",
      "Nearest to i:  consent influential patterns junta dakota wonder jay leap\n",
      "Nearest to six:  four eight three five nine one seven zero\n",
      "Nearest to nine:  one eight six seven zero five wa births\n",
      "Nearest to there:  injection actor wonder napoleon rand particles deposed edge\n",
      "Nearest to called:  suicide aikido believe issued computer reproductive atheism regulate\n",
      "Nearest to during:  rebels sample methodology legislature apparent bees ethiopia flags\n",
      "Nearest to more:  cathode maintains acupuncture goes prove cheap christ biography\n",
      "Nearest to for:  from in designer record antiquity immediately unique by\n",
      "\n",
      "\n",
      "Average loss at step 162000 :  4.771639768481254\n",
      "Average loss at step 164000 :  4.7484260554313655\n",
      "Average loss at step 166000 :  4.74339852464199\n",
      "Average loss at step 168000 :  4.76796884727478\n",
      "Average loss at step 170000 :  4.78723138821125\n",
      "Nearest to no:  helps often poll el they pixel sea drama\n",
      "Nearest to states:  certainly heard electron himself conviction parents spellings tech\n",
      "Nearest to be:  hellenistic companion selected scotland useful pakistani whose zones\n",
      "Nearest to these:  coup holiday italian served pascal doctrine multimedia napoleon\n",
      "Nearest to may:  physician sullivan we performer fossil animated apple soap\n",
      "Nearest to was:  is were hebrew labour missions friedrich channels being\n",
      "Nearest to when:  included especially einstein maryland codified peers discs worshipped\n",
      "Nearest to such:  chomsky flanders wwii wine updated parties learned utilize\n",
      "Nearest to i:  consent influential patterns junta dakota jay wonder leap\n",
      "Nearest to six:  four eight three five one nine seven zero\n",
      "Nearest to nine:  one eight six seven zero five wa births\n",
      "Nearest to there:  injection wonder actor napoleon rand particles edge deposed\n",
      "Nearest to called:  suicide aikido believe issued computer reproductive atheism regulate\n",
      "Nearest to during:  rebels legislature sample methodology apparent bees ethiopia flags\n",
      "Nearest to more:  cathode maintains acupuncture cheap prove goes christ optimal\n",
      "Nearest to for:  from in designer antiquity record immediately unique somewhere\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 172000 :  4.770021822094917\n",
      "Average loss at step 174000 :  4.743802294373512\n",
      "Average loss at step 176000 :  4.773569930553436\n",
      "Average loss at step 178000 :  4.760605023145676\n",
      "Average loss at step 180000 :  4.772156841158867\n",
      "Nearest to no:  helps often el poll they pixel sea drama\n",
      "Nearest to states:  certainly heard electron himself conviction parents spellings tech\n",
      "Nearest to be:  hellenistic companion selected scotland pakistani useful whose zones\n",
      "Nearest to these:  coup italian holiday served pascal doctrine multimedia napoleon\n",
      "Nearest to may:  physician sullivan we performer fossil animated pretty apple\n",
      "Nearest to was:  is were labour missions hebrew friedrich being abilities\n",
      "Nearest to when:  included especially einstein maryland codified peers discs worshipped\n",
      "Nearest to such:  chomsky flanders wwii wine learned updated parties utilize\n",
      "Nearest to i:  consent influential patterns junta dakota jay wonder leap\n",
      "Nearest to six:  four eight three five one nine seven zero\n",
      "Nearest to nine:  one eight seven six five zero wa births\n",
      "Nearest to there:  injection wonder actor napoleon edge rand particles deposed\n",
      "Nearest to called:  suicide aikido believe issued computer reproductive atheism regulate\n",
      "Nearest to during:  rebels legislature sample methodology apparent flags ethiopia bees\n",
      "Nearest to more:  cathode maintains acupuncture cheap prove christ goes optimal\n",
      "Nearest to for:  from designer in antiquity record immediately hemisphere somewhere\n",
      "\n",
      "\n",
      "Average loss at step 182000 :  4.767407323598862\n",
      "Average loss at step 184000 :  4.772974535346031\n",
      "Average loss at step 186000 :  4.720847477197647\n",
      "Average loss at step 188000 :  4.798863117694855\n",
      "Average loss at step 190000 :  4.7779906551837925\n",
      "Nearest to no:  helps often el poll they pixel drama sea\n",
      "Nearest to states:  certainly heard electron himself conviction spellings parents tech\n",
      "Nearest to be:  hellenistic companion selected scotland pakistani useful whose zones\n",
      "Nearest to these:  coup italian holiday served pascal doctrine multimedia napoleon\n",
      "Nearest to may:  physician sullivan we performer fossil animated pretty apple\n",
      "Nearest to was:  is were missions labour hebrew friedrich being abilities\n",
      "Nearest to when:  included especially einstein maryland codified peers discs worshipped\n",
      "Nearest to such:  chomsky flanders wwii wine learned utilize updated parties\n",
      "Nearest to i:  consent influential patterns junta dakota jay wonder leap\n",
      "Nearest to six:  four eight three five seven one zero nine\n",
      "Nearest to nine:  one eight seven six zero five wa births\n",
      "Nearest to there:  injection wonder actor napoleon edge rand desperate deposed\n",
      "Nearest to called:  suicide aikido believe issued computer reproductive atheism regulate\n",
      "Nearest to during:  legislature rebels sample methodology flags apparent wall bees\n",
      "Nearest to more:  maintains cathode cheap acupuncture christ prove optimal goes\n",
      "Nearest to for:  from antiquity in designer record hemisphere immediately tones\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(word_indexes, batch_size, num_skips, skip_window)\n",
    "        \n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        Ù€, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            \n",
    "            print(\"Average loss at step\", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "        \n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            \n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reversed_dictonary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbour \n",
    "\n",
    "                nearest = (-sim[i, :]).argsort()[1: top_k+1]\n",
    "                log_str = 'Nearest to %s: ' % valid_word\n",
    "                \n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reversed_dictonary[nearest[k]]\n",
    "                    log_str = '%s %s' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            print(\"\\n\")\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
